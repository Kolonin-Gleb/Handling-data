{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNEloi64OE2NTKxkNUwjdpp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LRVC9n21ghOc","executionInfo":{"status":"ok","timestamp":1680448169575,"user_tz":-180,"elapsed":5,"user":{"displayName":"Глеб Колонин","userId":"16800121630135430864"}},"outputId":"dfd7316e-495e-4161-c37c-e0b252a110e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Размер словаря: 9\n","Самые часто встречающиеся слова:\n","[('the', 1), ('this', 2), ('is', 3), ('document', 4), ('first', 5), ('second', 6), ('and', 7), ('third', 8), ('one', 9)]\n"]}],"source":["import tensorflow as tf\n","import json\n","\n","# Create a sample corpus\n","corpus = [\n","    'This is the first document.',\n","    'This is the second second document.',\n","    'And the third one.',\n","    'Is this the first document?',\n","]\n","\n","# Create and fit the tokenizer\n","tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","tokenizer.fit_on_texts(corpus)\n","items = list(tokenizer.word_index.items()) # Получаем индексы слов\n","\n","print(\"Размер словаря:\", len(items))\n","print(\"Самые часто встречающиеся слова:\")\n","print(items[:])"]},{"cell_type":"code","source":["# Save the tokenizer to a file\n","tokenizer_json = tokenizer.to_json()\n","with open('tokenizer.json', 'w', encoding='utf-8') as f:\n","    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"],"metadata":{"id":"wl53gXyXhKdy","executionInfo":{"status":"ok","timestamp":1680448210444,"user_tz":-180,"elapsed":4,"user":{"displayName":"Глеб Колонин","userId":"16800121630135430864"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Load the tokenizer from the file\n","with open('tokenizer.json', 'r', encoding='utf-8') as f:\n","    tokenizer_json = json.load(f)\n","    tokenizer2 = tf.keras.preprocessing.text.tokenizer_from_json(tokenizer_json)\n","\n","items2 = list(tokenizer2.word_index.items()) # Получаем индексы слов\n","print(\"Размер словаря:\", len(items))\n","print(\"Самые часто встречающиеся слова:\")\n","print(items[:])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ptgr1vkrgj3P","executionInfo":{"status":"ok","timestamp":1680448213316,"user_tz":-180,"elapsed":415,"user":{"displayName":"Глеб Колонин","userId":"16800121630135430864"}},"outputId":"94353ea0-5682-40db-ad16-2ac994c60538"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Размер словаря: 9\n","Самые часто встречающиеся слова:\n","[('the', 1), ('this', 2), ('is', 3), ('document', 4), ('first', 5), ('second', 6), ('and', 7), ('third', 8), ('one', 9)]\n"]}]}]}